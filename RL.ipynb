{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f670638",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "3626ca8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################ Environment ################\n",
    "\n",
    "import numpy as np\n",
    "import contextlib\n",
    "\n",
    "# Configures numpy print options\n",
    "@contextlib.contextmanager\n",
    "def _printoptions(*args, **kwargs):\n",
    "    original = np.get_printoptions()\n",
    "    np.set_printoptions(*args, **kwargs)\n",
    "    try:\n",
    "        yield\n",
    "    finally: \n",
    "        np.set_printoptions(**original)\n",
    "\n",
    "        \n",
    "class EnvironmentModel:\n",
    "    def __init__(self, n_states, n_actions, seed=None):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.random_state = np.random.RandomState(seed)\n",
    "        \n",
    "    def p(self, next_state, state, action):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def r(self, next_state, state, action):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def draw(self, state, action):\n",
    "        p = [self.p(ns, state, action) for ns in range(self.n_states)]\n",
    "        next_state = self.random_state.choice(self.n_states, p=p)\n",
    "        reward = self.r(next_state, state, action)\n",
    "        \n",
    "        return next_state, reward\n",
    "\n",
    "        \n",
    "class Environment(EnvironmentModel):\n",
    "    def __init__(self, n_states, n_actions, max_steps, pi, seed=None):\n",
    "        EnvironmentModel.__init__(self, n_states, n_actions, seed)\n",
    "        \n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        self.pi = pi\n",
    "        if self.pi is None:\n",
    "            self.pi = np.full(n_states, 1./n_states)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.n_steps = 0\n",
    "        self.state = self.random_state.choice(self.n_states, p=self.pi)\n",
    "        \n",
    "        return self.state\n",
    "        \n",
    "    def step(self, action):\n",
    "        if action < 0 or action >= self.n_actions:\n",
    "            raise Exception('Invalid action.')\n",
    "        \n",
    "        self.n_steps += 1\n",
    "        done = (self.n_steps >= self.max_steps)\n",
    "        \n",
    "        self.state, reward = self.draw(self.state, action)\n",
    "        \n",
    "        return self.state, reward, done\n",
    "    \n",
    "    def render(self, policy=None, value=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        \n",
    "class FrozenLake(Environment):\n",
    "    def __init__(self, lake, slip, max_steps, seed=None):\n",
    "        \"\"\"\n",
    "        lake: A matrix that represents the lake. For example:\n",
    "         lake =  [['&', '.', '.', '.'],\n",
    "                  ['.', '#', '.', '#'],\n",
    "                  ['.', '.', '.', '#'],\n",
    "                  ['#', '.', '.', '$']]\n",
    "        slip: The probability that the agent will slip\n",
    "        max_steps: The maximum number of time steps in an episode\n",
    "        seed: A seed to control the random number generator (optional)\n",
    "        \"\"\"\n",
    "        # start (&), frozen (.), hole (#), goal ($)\n",
    "        self.lake = np.array(lake)\n",
    "        self.lake_flat = self.lake.reshape(-1)\n",
    "        \n",
    "        self.slip = slip\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        self.n_states = self.lake.size + 1\n",
    "        self.n_actions = 4\n",
    "        \n",
    "        self.pi = np.zeros(self.n_states, dtype=float)\n",
    "        self.pi[np.where(self.lake_flat == '&')[0]] = 1.0\n",
    "        \n",
    "        self.pin = np.zeros(self.n_states, dtype=float)\n",
    "        self.pin[np.where(self.lake_flat == '&')[0]] = 1.0\n",
    "        self.pin[np.where(self.lake_flat == '#')[0]] = -1.0\n",
    "        \n",
    "        self.holes = []\n",
    "        i = 0\n",
    "        for index in self.pin:\n",
    "            if index == -1:\n",
    "                self.holes.append(i)\n",
    "            i += 1\n",
    "        \n",
    "        self.absorbing_state = self.n_states - 1\n",
    "        \n",
    "        # TODO:\n",
    "        self.rows = np.sqrt(self.n_states-1)\n",
    "        \n",
    "        self.random_state = np.random.RandomState(seed)\n",
    "        \n",
    "    def step(self, action):\n",
    "        state, reward, done = Environment.step(self, action)\n",
    "        \n",
    "        done = (state == self.absorbing_state) or done\n",
    "        \n",
    "        return state, reward, done\n",
    "        \n",
    "    def p(self, next_state, state, action):\n",
    "        \n",
    "        # TODO:\n",
    "        prob = 0\n",
    "        \n",
    "        if state in self.holes or state == self.rows*self.rows - 1 or state == self.rows*self.rows:\n",
    "            if next_state == self.rows*self.rows:\n",
    "                prob += 1\n",
    "            return prob\n",
    "#         elif state == self.rows*self.rows:\n",
    "#             if next_state == self.rows*self.rows:\n",
    "#                 prob += 1\n",
    "                \n",
    "        else:\n",
    "            if next_state == state:\n",
    "                if state < self.rows:\n",
    "                    if state == 0:\n",
    "                        prob += self.slip/2\n",
    "                        if action == 0 or action == 1:\n",
    "                            prob += 1-self.slip\n",
    "                    elif state == self.rows - 1:\n",
    "                        prob += self.slip/2\n",
    "                        if action == 0 or action == 3:\n",
    "                            prob += 1-self.slip\n",
    "                    else:\n",
    "                        prob += self.slip/4\n",
    "                        if action == 0:\n",
    "                            prob += 1-self.slip\n",
    "                elif state%self.rows == 0 and not state == self.rows*self.rows:\n",
    "                    if state == self.rows*(self.rows-1):\n",
    "                        prob += self.slip/2\n",
    "                        if action == 1 or action == 2:\n",
    "                            prob += 1-self.slip\n",
    "                    else:\n",
    "                        prob += self.slip/4\n",
    "                        if action == 1:\n",
    "                            prob += 1-self.slip\n",
    "                elif state%self.rows == self.rows-1:\n",
    "                    if state == self.rows*(self.rows)-1:\n",
    "                        prob += self.slip/2\n",
    "                        if action == 2 or action == 3:\n",
    "                            prob += 1-self.slip\n",
    "                    else:\n",
    "                        prob += self.slip/4\n",
    "                        if action == 3:\n",
    "                            prob += 1-self.slip\n",
    "                elif state > self.rows*(self.rows-1) and state < self.rows*self.rows-1:\n",
    "                    prob += self.slip/4\n",
    "                    if action == 2:\n",
    "                        prob += 1-self.slip\n",
    "\n",
    "            if next_state - state == 1 and not next_state%self.rows == 0 :\n",
    "                if action == 3:\n",
    "                    prob += 1 - self.slip*3/4\n",
    "                else:\n",
    "                    prob += self.slip/4\n",
    "            if next_state - state == -1 and not state%self.rows == 0:\n",
    "                if action == 1:\n",
    "                    prob += 1 - self.slip*3/4\n",
    "                else:\n",
    "                    prob += self.slip/4\n",
    "            if next_state - state == self.rows and not next_state == self.rows*self.rows:\n",
    "                if action == 2:\n",
    "                    prob += 1 - self.slip*3/4\n",
    "                else:\n",
    "                    prob += self.slip/4\n",
    "            if next_state - state == -self.rows:\n",
    "                if action == 0:\n",
    "                    prob += 1 - self.slip*3/4\n",
    "                else:\n",
    "                    prob += self.slip/4\n",
    "                \n",
    "        return prob\n",
    "                \n",
    "                \n",
    "    \n",
    "    def r(self, next_state, state, action):\n",
    "        # TODO:\n",
    "        reward = 0\n",
    "        if next_state == self.rows*self.rows:\n",
    "            if state == self.rows*self.rows - 1:\n",
    "                reward += 1 \n",
    "        return reward\n",
    "   \n",
    "    def render(self, policy=None, value=None):\n",
    "        if policy is None:\n",
    "            lake = np.array(self.lake_flat)\n",
    "            \n",
    "            if self.state < self.absorbing_state:\n",
    "                lake[self.state] = '@'\n",
    "                \n",
    "            print(lake.reshape(self.lake.shape))\n",
    "        else:\n",
    "            # UTF-8 arrows look nicer, but cannot be used in LaTeX\n",
    "            # https://www.w3schools.com/charsets/ref_utf_arrows.asp\n",
    "            actions = ['^', '<', '_', '>']\n",
    "            \n",
    "            print('Lake:')\n",
    "            print(self.lake)\n",
    "        \n",
    "            print('Policy:')\n",
    "            policy = np.array([actions[a] for a in policy[:-1]])\n",
    "            print(policy.reshape(self.lake.shape))\n",
    "            \n",
    "            print('Value:')\n",
    "            with _printoptions(precision=3, suppress=True):\n",
    "                print(value[:-1].reshape(self.lake.shape))\n",
    "                \n",
    "    def reset(self):\n",
    "        self.n_steps = 0\n",
    "        self.state = self.random_state.choice(self.n_states, p=self.pi)\n",
    "        \n",
    "        return self.state\n",
    "                \n",
    "def play(env):\n",
    "    actions = ['w', 'a', 's', 'd']\n",
    "    \n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        c = input('\\nMove: ')\n",
    "        if c not in actions:\n",
    "            raise Exception('Invalid action')\n",
    "            \n",
    "        state, r, done = env.step(actions.index(c))\n",
    "        \n",
    "        env.render()\n",
    "        print('Reward: {0}.'.format(r))\n",
    "\n",
    "################ Model-based algorithms ################\n",
    "\n",
    "def policy_evaluation(env, policy, gamma, theta, max_iterations):\n",
    "    val = np.zeros(env.n_states, dtype=np.float)\n",
    "    for i in range (max_iterations):\n",
    "        delta = 0\n",
    "        for s in range (env.n_states):\n",
    "            v = 0\n",
    "            for ns in range (env.n_states):\n",
    "                v += env.p(ns, s, policy[s])*(env.r(ns, s, policy[s])+gamma*val[ns])\n",
    "            delta = max(delta, abs(v-val[s]))\n",
    "            val[s] = v\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return val\n",
    "    \n",
    "def policy_improvement(env, policy, value, gamma):\n",
    "    \n",
    "    policy_new = np.copy(policy)\n",
    "    \n",
    "    for s in range(env.n_states):\n",
    "        bestAction = 0\n",
    "        bestValue = 0\n",
    "        for a in range(env.n_actions):\n",
    "            result = 0\n",
    "            for ns in range(env.n_states):\n",
    "                result += env.p(ns, s, a)*(env.r(ns, s, a)+gamma*value[ns])\n",
    "            if result>bestValue:\n",
    "                bestAction = a\n",
    "                bestValue = result\n",
    "        policy_new[s] = bestAction\n",
    "    return policy_new\n",
    "\n",
    "    \n",
    "def policy_iteration(env, gamma, theta, max_iterations, policy=None):\n",
    "    if policy is None:\n",
    "        policy = np.zeros(env.n_states, dtype=int)\n",
    "    else:\n",
    "        policy = np.array(policy, dtype=int)\n",
    "        \n",
    "    iters = 0\n",
    "    for i in range(max_iterations):\n",
    "        iters += 1\n",
    "        value = policy_evaluation (env, policy, gamma, theta, max_iterations)\n",
    "        policy_new = policy_improvement(env, policy, value, gamma)\n",
    "        if np.array_equal(policy_new, policy): \n",
    "            break\n",
    "        policy = policy_new\n",
    "    return policy, value, iters\n",
    "        \n",
    "    \n",
    "def value_iteration(env, gamma, theta, max_iterations, value=None):\n",
    "    if value is None:\n",
    "        value = np.zeros(env.n_states)\n",
    "    else:\n",
    "        value = np.array(value, dtype=np.float)\n",
    "        \n",
    "    policy = np.zeros(env.n_states, dtype=np.int)\n",
    "    \n",
    "    iters = 0\n",
    "    \n",
    "    #Value optimization\n",
    "    \n",
    "    for i in range (max_iterations):\n",
    "        iters += 1\n",
    "        delta = 0\n",
    "        for s in range(env.n_states):\n",
    "            val = 0\n",
    "            for a in range(env.n_actions):\n",
    "                result = 0\n",
    "                for ns in range(env.n_states):\n",
    "                    result += env.p(ns, s, a)*(env.r(ns, s, a)+gamma*value[ns])\n",
    "                val = max(result, val)\n",
    "            delta = max(delta, abs(val-value[s]))\n",
    "            value[s] = val\n",
    "        if delta < theta:\n",
    "            break\n",
    "            \n",
    "    #Policy retrival\n",
    "    for s in range(env.n_states):\n",
    "        val = 0\n",
    "        bestAction = 0\n",
    "        for a in range(env.n_actions):\n",
    "            result = 0\n",
    "            for ns in range(env.n_states):\n",
    "                result += env.p(ns, s, a)*(env.r(ns, s, a)+gamma*value[ns])\n",
    "            if result > val:\n",
    "                val = result\n",
    "                bestAction = a\n",
    "        policy[s] = int(bestAction)\n",
    "            \n",
    "    return policy, value, iters\n",
    "\n",
    "################ Tabular model-free algorithms ################\n",
    "\n",
    "def epsilon_greedy(env, q, epsilon, state, random_state):\n",
    "    \n",
    "    bestAction = np.argmax(q[state, :])\n",
    "    p = np.zeros(env.n_actions)\n",
    "    for i in range(env.n_actions):\n",
    "        if i == bestAction: p[i]=1-epsilon\n",
    "        else: p[i]=epsilon/(env.n_actions-1)                          \n",
    "    return random_state.choice(env.n_actions, p=p)\n",
    "    \n",
    "#     if np.random.random() < epsilon:\n",
    "#           return np.random.randint(env.n_actions)\n",
    "#     else:\n",
    "#           return np.argmax(q[state, :])\n",
    "\n",
    "def sarsa(env, max_episodes, eta, gamma, epsilon, optimalValue, seed=None):\n",
    "\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    \n",
    "    eta = np.linspace(eta, 0, max_episodes)\n",
    "    epsilon = np.linspace(epsilon, 0, max_episodes)\n",
    "    \n",
    "    q = np.zeros((env.n_states, env.n_actions))\n",
    "    \n",
    "    iters = 0\n",
    "    for i in range(max_episodes):\n",
    "        iters +=1\n",
    "        s = env.reset()\n",
    "        # TODO:\n",
    "        a = epsilon_greedy(env, q, epsilon[i], s, random_state)\n",
    "        done = False\n",
    "        while not done:\n",
    "            ns, reward, done = env.step(a)\n",
    "            na = epsilon_greedy(env, q, epsilon[i], ns, random_state)\n",
    "            q[s, a] += eta[i] * (reward + (gamma * q[ns, na]) - q[s, a])\n",
    "            s = ns\n",
    "            a = na\n",
    "            \n",
    "        policy = q.argmax(axis=1)\n",
    "        valueForPolicy = policy_evaluation(env, policy, gamma, 0.01, 100)\n",
    "        if np.array_equal(valueForPolicy, optimalValue):\n",
    "            print(iters)\n",
    "            break\n",
    "            \n",
    "    policy = q.argmax(axis=1)   \n",
    "    value = q.max(axis=1)\n",
    "        \n",
    "    return policy, value\n",
    "    \n",
    "def q_learning(env, max_episodes, eta, gamma, epsilon, optimalValue, seed=None):\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    \n",
    "    eta = np.linspace(eta, 0, max_episodes)\n",
    "    epsilon = np.linspace(epsilon, 0, max_episodes)\n",
    "    \n",
    "    q = np.zeros((env.n_states, env.n_actions))\n",
    "    \n",
    "    iters = 0\n",
    "    for i in range(max_episodes):\n",
    "        iters +=1\n",
    "        s = env.reset()\n",
    "        # TODO:\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = epsilon_greedy(env, q, epsilon[i], s, random_state)\n",
    "            ns, reward, done = env.step(a)\n",
    "            maxQ = 0\n",
    "            for action in range(env.n_actions):\n",
    "                if q[ns, action]>maxQ: maxQ = q[ns, action]\n",
    "            q[s, a] += eta[i] * (reward + (gamma * maxQ) - q[s, a])\n",
    "            s = ns\n",
    "\n",
    "        policy = q.argmax(axis=1)\n",
    "        valueForPolicy = policy_evaluation(env, policy, gamma, 0.01, 100)\n",
    "        if np.array_equal(valueForPolicy, optimalValue):\n",
    "            print(iters)\n",
    "            break\n",
    "        \n",
    "#     policy = q.argmax(axis=1)\n",
    "    value = q.max(axis=1)\n",
    "        \n",
    "    return policy, value\n",
    "\n",
    "################ Non-tabular model-free algorithms ################\n",
    "\n",
    "class LinearWrapper:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        \n",
    "        self.n_actions = self.env.n_actions\n",
    "        self.n_states = self.env.n_states\n",
    "        self.n_features = self.n_actions * self.n_states\n",
    "        \n",
    "    def encode_state(self, s):\n",
    "        features = np.zeros((self.n_actions, self.n_features))\n",
    "        for a in range(self.n_actions):\n",
    "            i = np.ravel_multi_index((s, a), (self.n_states, self.n_actions))\n",
    "            features[a, i] = 1.0\n",
    "          \n",
    "        return features\n",
    "    \n",
    "    def decode_policy(self, theta):\n",
    "        policy = np.zeros(self.env.n_states, dtype=int)\n",
    "        value = np.zeros(self.env.n_states)\n",
    "        \n",
    "        for s in range(self.n_states):\n",
    "            features = self.encode_state(s)\n",
    "            q = features.dot(theta)\n",
    "            \n",
    "            policy[s] = np.argmax(q)\n",
    "            value[s] = np.max(q)\n",
    "        \n",
    "        return policy, value\n",
    "        \n",
    "    def reset(self):\n",
    "        return self.encode_state(self.env.reset())\n",
    "    \n",
    "    def step(self, action):\n",
    "        state, reward, done = self.env.step(action)\n",
    "        \n",
    "        return self.encode_state(state), reward, done\n",
    "    \n",
    "    def render(self, policy=None, value=None):\n",
    "        self.env.render(policy, value)\n",
    "        \n",
    "def epsilon_greedy_linear(env, q, epsilon, random_state):\n",
    "    \n",
    "    bestAction = np.argmax(q[:])\n",
    "    p = np.zeros(env.n_actions)\n",
    "    for i in range(env.n_actions):\n",
    "        if i == bestAction: p[i]=1-epsilon\n",
    "        else: p[i]=epsilon/(env.n_actions-1)                          \n",
    "    return random_state.choice(env.n_actions, p=p)\n",
    "        \n",
    "def linear_sarsa(env, max_episodes, eta, gamma, epsilon, seed=None):\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    \n",
    "    eta = np.linspace(eta, 0, max_episodes)\n",
    "    epsilon = np.linspace(epsilon, 0, max_episodes)\n",
    "    \n",
    "    theta = np.zeros(env.n_features)\n",
    "    \n",
    "    for i in range(max_episodes):\n",
    "        features = env.reset()\n",
    "        \n",
    "        q = features.dot(theta)\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            a = epsilon_greedy_linear(env, q, epsilon[i], random_state)\n",
    "            nfeatures, r, done = env.step(a)\n",
    "            delta = r - q[a]\n",
    "            q = np.dot(nfeatures, theta)\n",
    "            na = epsilon_greedy_linear(env, q, epsilon[i], random_state)\n",
    "            delta += gamma * q[na]\n",
    "            theta += eta[i] * delta * features[a]\n",
    "            features = nfeatures\n",
    "            a = na\n",
    "    \n",
    "    return theta\n",
    "    \n",
    "def linear_q_learning(env, max_episodes, eta, gamma, epsilon, seed=None):\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    \n",
    "    eta = np.linspace(eta, 0, max_episodes)\n",
    "    epsilon = np.linspace(epsilon, 0, max_episodes)\n",
    "    \n",
    "    theta = np.zeros(env.n_features)\n",
    "    \n",
    "    for i in range(max_episodes):\n",
    "        features = env.reset()\n",
    "        \n",
    "        q = features.dot(theta)\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            a = epsilon_greedy_linear(env, q, epsilon[i], random_state)\n",
    "            features_prime, r, done = env.step(a)\n",
    "            delta = r - q[a]\n",
    "            q = np.dot(features_prime, theta)\n",
    "            maxQ = 0\n",
    "            for action in range(env.n_actions):\n",
    "                if q[action]>maxQ: maxQ = q[action]\n",
    "            delta += gamma * maxQ\n",
    "            theta += eta[i] * delta * features[a]\n",
    "            features = features_prime\n",
    "\n",
    "    return theta    \n",
    "\n",
    "################ Main function ################\n",
    "\n",
    "def main():\n",
    "    seed = 0\n",
    "    \n",
    "    # Small lake\n",
    "    lake =   [['&', '.', '.', '.'],\n",
    "              ['.', '#', '.', '#'],\n",
    "              ['.', '.', '.', '#'],\n",
    "              ['#', '.', '.', '$']]\n",
    "\n",
    "    env = FrozenLake(lake, slip=0.1, max_steps=16, seed=seed)\n",
    "    \n",
    "    print('# Model-based algorithms')\n",
    "    gamma = 0.9\n",
    "    theta = 0.001\n",
    "    max_iterations = 100\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    print('## Policy iteration')\n",
    "    policy, value, iters = policy_iteration(env, gamma, theta, max_iterations)\n",
    "    env.render(policy, value)\n",
    "    print(\"Number of iterations: \"+ str(iters))\n",
    "    print('')\n",
    "    \n",
    "    print('## Value iteration')\n",
    "    policy, value, iters = value_iteration(env, gamma, theta, max_iterations)\n",
    "    env.render(policy, value)\n",
    "    print(\"Number of iterations: \"+ str(iters))\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    print('# Model-free algorithms')\n",
    "    max_episodes = 200000\n",
    "    eta = 0.5\n",
    "    epsilon = 0.5\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    print('## Sarsa')\n",
    "    policy, value = sarsa(env, max_episodes, eta, gamma, epsilon, seed=seed)\n",
    "    env.render(policy, value)\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    print('## Q-learning')\n",
    "    policy, value = q_learning(env, max_episodes, eta, gamma, epsilon, seed=seed)\n",
    "    env.render(policy, value)\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    linear_env = LinearWrapper(env)\n",
    "    \n",
    "    print('## Linear Sarsa')\n",
    "    \n",
    "    parameters = linear_sarsa(linear_env, max_episodes, eta,\n",
    "                              gamma, epsilon, seed=seed)\n",
    "    policy, value = linear_env.decode_policy(parameters)\n",
    "    linear_env.render(policy, value)\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    print('## Linear Q-learning')\n",
    "    \n",
    "    parameters = linear_q_learning(linear_env, max_episodes, eta,\n",
    "                                   gamma, epsilon, seed=seed)\n",
    "    policy, value = linear_env.decode_policy(parameters)\n",
    "    linear_env.render(policy, value)\n",
    "\n",
    "def main_large():\n",
    "    seed = 0\n",
    "    \n",
    "    # Large lake\n",
    "    lake =   [['&', '.', '.', '.', '.', '.', '.', '.'],\n",
    "              ['.', '.', '.', '.', '.', '.', '.', '.'],\n",
    "              ['.', '.', '.', '#', '.', '.', '.', '.'],\n",
    "              ['.', '.', '.', '.', '.', '#', '.', '.'],\n",
    "              ['.', '.', '.', '#', '.', '.', '.', '.'],\n",
    "              ['.', '#', '#', '.', '.', '.', '#', '.'],\n",
    "              ['.', '#', '.', '.', '#', '.', '#', '.'],\n",
    "              ['.', '.', '.', '#', '.', '.', '.', '$']]\n",
    "\n",
    "    env = FrozenLake(lake, slip=0.1, max_steps=16, seed=seed)\n",
    "    \n",
    "    print('# Model-based algorithms')\n",
    "    gamma = 0.9\n",
    "    theta = 0.001\n",
    "    max_iterations = 100\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    print('## Policy iteration')\n",
    "    policy, value = policy_iteration(env, gamma, theta, max_iterations)\n",
    "    env.render(policy, value)\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    print('## Value iteration')\n",
    "    policy, value = value_iteration(env, gamma, theta, max_iterations)\n",
    "    env.render(policy, value)\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    print('# Model-free algorithms')\n",
    "    max_episodes = 2000\n",
    "    eta = 0.5\n",
    "    epsilon = 0.5\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    print('## Sarsa')\n",
    "    policy, value = sarsa(env, max_episodes, eta, gamma, epsilon, seed=seed)\n",
    "    env.render(policy, value)\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    print('## Q-learning')\n",
    "    policy, value = q_learning(env, max_episodes, eta, gamma, epsilon, seed=seed)\n",
    "    env.render(policy, value)\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    linear_env = LinearWrapper(env)\n",
    "    \n",
    "    print('## Linear Sarsa')\n",
    "    \n",
    "    parameters = linear_sarsa(linear_env, max_episodes, eta,\n",
    "                              gamma, epsilon, seed=seed)\n",
    "    policy, value = linear_env.decode_policy(parameters)\n",
    "    linear_env.render(policy, value)\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    print('## Linear Q-learning')\n",
    "    \n",
    "    parameters = linear_q_learning(linear_env, max_episodes, eta,\n",
    "                                   gamma, epsilon, seed=seed)\n",
    "    policy, value = linear_env.decode_policy(parameters)\n",
    "    linear_env.render(policy, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "aa587f2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "    \n",
    "# Small lake\n",
    "lake =   [['&', '.', '.', '.'],\n",
    "          ['.', '#', '.', '#'],\n",
    "          ['.', '.', '.', '#'],\n",
    "          ['#', '.', '.', '$']]\n",
    "\n",
    "env = FrozenLake(lake, slip=0.1, max_steps=100, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "dc39b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range (17):\n",
    "    sumof = 0 \n",
    "    for i in range(17):\n",
    "        sumof += env.p(i,j,3)\n",
    "    if not sumof == 1.0:\n",
    "        print(sumof, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "1e2743fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.p(6,5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "9d1c3701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-662-f9d837c93c73>:253: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  val = np.zeros(env.n_states, dtype=np.float)\n"
     ]
    }
   ],
   "source": [
    "_, value, _ = policy_iteration(env, 0.9, 0.01, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "81077df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-662-f9d837c93c73>:253: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  val = np.zeros(env.n_states, dtype=np.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([2, 3, 2, 1, 2, 0, 2, 0, 3, 2, 2, 0, 0, 3, 3, 2, 0]),\n",
       " array([0.01341984, 0.02940991, 0.01892601, 0.01154619, 0.03946482,\n",
       "        0.        , 0.20411077, 0.        , 0.05141579, 0.08845898,\n",
       "        0.25184301, 0.        , 0.        , 0.33342023, 0.77470978,\n",
       "        0.99942679, 0.        ]))"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa(env, 20000, 0.5, 0.9, 0.5, value, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "d46acec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-662-f9d837c93c73>:253: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  val = np.zeros(env.n_states, dtype=np.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([2, 3, 2, 1, 2, 0, 2, 0, 3, 2, 2, 0, 0, 3, 3, 0, 0]),\n",
       " array([0.49287918, 0.48811403, 0.546153  , 0.47842626, 0.55752958,\n",
       "        0.        , 0.58983145, 0.        , 0.63440045, 0.72372404,\n",
       "        0.69095815, 0.        , 0.        , 0.80978268, 0.89999719,\n",
       "        1.        , 0.        ]))"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_learning(env, 20000, 0.5, 0.9, 0.5, value, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "02ef79a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-662-f9d837c93c73>:307: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  policy = np.zeros(env.n_states, dtype=np.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([2, 3, 2, 1, 2, 0, 2, 0, 3, 2, 2, 0, 0, 3, 3, 0, 0]),\n",
       " array([0.45445927, 0.50358629, 0.57904696, 0.50476979, 0.50802096,\n",
       "        0.        , 0.65265184, 0.        , 0.5842415 , 0.67227225,\n",
       "        0.76831633, 0.        , 0.        , 0.77097869, 0.88709375,\n",
       "        1.        , 0.        ]),\n",
       " 10)"
      ]
     },
     "execution_count": 669,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_iteration(env, 0.9, 0.01, 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "d2a2dca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 3, 2, 1, 2, 0, 2, 0, 3, 2, 2, 0, 0, 3, 3, 2, 0]),\n",
       " array([0.42626583, 0.38584404, 0.47352703, 0.31483086, 0.48382264,\n",
       "        0.        , 0.58289165, 0.        , 0.53801505, 0.64805014,\n",
       "        0.7202116 , 0.        , 0.        , 0.75037442, 0.87733795,\n",
       "        1.        , 0.        ]))"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa(env, 2000, 0.5, 0.9, 0.5, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "6ffa3570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3, 3, 2, 1, 0, 0, 2, 0, 0, 3, 2, 0, 0, 3, 3, 1, 0]),\n",
       " array([0.44506135, 0.51224076, 0.59070131, 0.50008675, 0.3898141 ,\n",
       "        0.        , 0.67083654, 0.        , 0.31950971, 0.66594854,\n",
       "        0.78180981, 0.        , 0.        , 0.77643929, 0.88959393,\n",
       "        1.        , 0.        ]))"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_learning(env, 2000, 0.5, 0.9, 0.5, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "d5aec818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Linear Sarsa\n",
      "[3 3 2 1 2 0 2 0 3 2 2 0 0 3 3 0 0]\n",
      "[0.40068182 0.45837644 0.54904965 0.37239854 0.38160776 0.\n",
      " 0.63738049 0.         0.4349277  0.63081756 0.76329268 0.\n",
      " 0.         0.75816945 0.87870467 1.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "linear_env = LinearWrapper(env)\n",
    "    \n",
    "print('## Linear Sarsa')\n",
    "\n",
    "parameters = linear_sarsa(linear_env, 2000, 0.5,\n",
    "                          0.9, 0.5, seed=0)\n",
    "policy, value = linear_env.decode_policy(parameters)\n",
    "print(policy)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "ba5bafce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 2 1 2 0 2 0 3 3 2 0 0 3 3 0 0]\n",
      "[0.45880241 0.49445097 0.5694942  0.49983582 0.51589666 0.\n",
      " 0.63861416 0.         0.59008092 0.67945124 0.77423918 0.\n",
      " 0.         0.7438747  0.88429343 1.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "parameters = linear_q_learning(linear_env, 20000, 0.5,\n",
    "                          0.9, 0.5, seed=0)\n",
    "policy, value = linear_env.decode_policy(parameters)\n",
    "print(policy)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "028dd6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "    \n",
    "# Large lake\n",
    "lake =   [['&', '.', '.', '.', '.', '.', '.', '.'],\n",
    "          ['.', '.', '.', '.', '.', '.', '.', '.'],\n",
    "          ['.', '.', '.', '#', '.', '.', '.', '.'],\n",
    "          ['.', '.', '.', '.', '.', '#', '.', '.'],\n",
    "          ['.', '.', '.', '#', '.', '.', '.', '.'],\n",
    "          ['.', '#', '#', '.', '.', '.', '#', '.'],\n",
    "          ['.', '#', '.', '.', '#', '.', '#', '.'],\n",
    "          ['.', '.', '.', '#', '.', '.', '.', '$']]\n",
    "\n",
    "envLarge = FrozenLake(lake, slip=0.1, max_steps=16, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "224ea5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "34ac024b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-662-f9d837c93c73>:253: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  val = np.zeros(env.n_states, dtype=np.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8734360000003107\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "_, val, _ = policy_iteration(envLarge, 0.9, 0.01, 500)\n",
    "print(time.process_time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "88c51116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-609-57c032933b85>:307: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  policy = np.zeros(env.n_states, dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6775689999999486\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "value_iteration(envLarge, 0.9, 0.01, 500)\n",
    "print(time.process_time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "cf6f1e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-662-f9d837c93c73>:253: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  val = np.zeros(env.n_states, dtype=np.float)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa(envLarge, 20000, 0.5, 0.9, 0.5, val, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a31f479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
